{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8n7XGkmKXTat"
      },
      "outputs": [],
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "!npm install -g localtunnel\n",
        "!pip install ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSqhi6KzXvvr"
      },
      "outputs": [],
      "source": [
        "import subprocess, os\n",
        "\n",
        "my_env = os.environ\n",
        "my_env[\"OLLAMA_HOST\"] = \"0.0.0.0\"\n",
        "ollama_proc = subprocess.Popen(\"ollama serve\", env=my_env, shell=True)\n",
        "ollama_proc.pid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7Tb8iqlkucB"
      },
      "outputs": [],
      "source": [
        "tunnel_cmd = \"npx localtunnel --port 11434\"\n",
        "tunnel_proc = subprocess.Popen(tunnel_cmd, shell=True, text=True, stdout=subprocess.PIPE)\n",
        "\n",
        "print(tunnel_proc.pid)\n",
        "print(tunnel_proc.stdout.readline())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e57vUjhCxfLh"
      },
      "outputs": [],
      "source": [
        "ollama_url = \"https://thick-ghosts-retire.loca.lt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7K_XPV_dYn3d"
      },
      "outputs": [],
      "source": [
        "!ollama pull llava\n",
        "!ollama pull llama3.2:1b\n",
        "!ollama list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfnRFNMVqZOH"
      },
      "source": [
        "### Prompt with Python (local)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NiU1yk4hZCdN"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "from ollama import chat\n",
        "\n",
        "img = base64.b64encode(Path(\"./bun.png\").read_bytes()).decode()\n",
        "\n",
        "response = chat(\n",
        "  model='llava',\n",
        "  messages=[\n",
        "    {\n",
        "      'role': 'user',\n",
        "      'content': 'What is in this image? Be concise.',\n",
        "      'images': [img],\n",
        "    }\n",
        "  ],\n",
        ")\n",
        "\n",
        "display(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxMI8t_iqwkJ"
      },
      "source": [
        "### Prompt with Python (remote)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import base64\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "from ollama import Client\n",
        "\n",
        "client = Client(host=ollama_url)\n",
        "\n",
        "img = base64.b64encode(Path(\"./bun.png\").read_bytes()).decode()\n",
        "\n",
        "response = client.chat(\n",
        "    model='llava',\n",
        "    messages=[{\n",
        "        'role': 'user',\n",
        "        'content': 'What is in this image? Be concise.',\n",
        "        'images': [img],\n",
        "    }]\n",
        ")\n",
        "\n",
        "display(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwCkSI2jZ2yi"
      },
      "outputs": [],
      "source": [
        "from ollama import Client\n",
        "\n",
        "client = Client(host=ollama_url)\n",
        "\n",
        "response = client.chat(\n",
        "    model='llama3.2:1b',\n",
        "    messages=[{\n",
        "        'role': 'user',\n",
        "        'content': 'Why is the sky blue?',\n",
        "    }]\n",
        ")\n",
        "\n",
        "display(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9ufkmVZrm5k"
      },
      "source": [
        "### Prompt with curl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udYZ6eb-rowr"
      },
      "outputs": [],
      "source": [
        "!curl https://free-apples-feel.loca.lt/api/generate -d \\\n",
        "'{\"model\":\"llama3.2:1b\",\\\n",
        "  \"prompt\": \"Why is the sky blue?\",\\\n",
        "  \"stream\": false}'"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
