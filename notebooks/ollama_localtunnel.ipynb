{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8n7XGkmKXTat"
      },
      "outputs": [],
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "!wget -P ~ https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
        "!dpkg -i ~/cloudflared-linux-amd64.deb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSqhi6KzXvvr"
      },
      "outputs": [],
      "source": [
        "import subprocess, os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "my_env = os.environ\n",
        "my_env[\"OLLAMA_HOST\"] = \"0.0.0.0\"\n",
        "ollama_proc = subprocess.Popen(\"ollama serve\", env=my_env, shell=True)\n",
        "\n",
        "print(ollama_proc.pid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!ollama pull llava\n",
        "!ollama pull llama3.2:1b\n",
        "!ollama list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!ps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7Tb8iqlkucB"
      },
      "outputs": [],
      "source": [
        "tunnel_cmd = \"cloudflared tunnel --url http://127.0.0.1:11434\"\n",
        "tunnel_proc = subprocess.Popen(tunnel_cmd, shell=True, text=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "\n",
        "print(tunnel_proc.pid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for _ in range(10):\n",
        "  print(tunnel_proc.stderr.readline(), end=\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e57vUjhCxfLh"
      },
      "outputs": [],
      "source": [
        "ollama_url = \"https://steel-exp-february-hired.trycloudflare.com\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfnRFNMVqZOH"
      },
      "source": [
        "### Prompt with Python (local)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NiU1yk4hZCdN"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "from ollama import chat\n",
        "\n",
        "img = base64.b64encode(Path(\"../imgs/bun.png\").read_bytes()).decode()\n",
        "\n",
        "response = chat(\n",
        "  model='llava',\n",
        "  messages=[\n",
        "    {\n",
        "      'role': 'user',\n",
        "      'content': 'What is in this image? Be concise.',\n",
        "      'images': [img],\n",
        "    }\n",
        "  ],\n",
        ")\n",
        "\n",
        "display(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxMI8t_iqwkJ"
      },
      "source": [
        "### Prompt with Python (remote)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import base64\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "from ollama import Client\n",
        "\n",
        "client = Client(host=ollama_url)\n",
        "\n",
        "img = base64.b64encode(Path(\"../imgs/bun.png\").read_bytes()).decode()\n",
        "\n",
        "response = client.chat(\n",
        "    model='llava',\n",
        "    messages=[{\n",
        "        'role': 'user',\n",
        "        'content': 'What is in this image? Be concise.',\n",
        "        'images': [img],\n",
        "    }]\n",
        ")\n",
        "\n",
        "display(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwCkSI2jZ2yi"
      },
      "outputs": [],
      "source": [
        "from ollama import Client\n",
        "\n",
        "client = Client(host=ollama_url)\n",
        "\n",
        "response = client.chat(\n",
        "    model='llama3.2:1b',\n",
        "    messages=[{\n",
        "        'role': 'user',\n",
        "        'content': 'Why is the sky blue?',\n",
        "    }]\n",
        ")\n",
        "\n",
        "display(response)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
